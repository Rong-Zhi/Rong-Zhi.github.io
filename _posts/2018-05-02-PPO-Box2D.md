---
layout: post
title: PPO Gym env
date: 2018-05-02
excerpt: "Here are some PPO implmented results in OpenAI Gym environments"
tags: [Reinforcement Learning]
comments: true
---

Here are some PPO implemented results in OpenAI Gym Box2D environments.

## 1. LunarLander Continuous-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95\\

### Results
Initally the agent takes random actions:
<img src="/images/PPO-lunarlander/videos/1.gif" height="407" width="649">

After 100 iterations, the agent starts to learn how to land:

<img src="/images/PPO-lunarlander/videos/100.gif" height="407" width="649">

After 400 iterations, the agent could land successfully:

<img src="/images/PPO-lunarlander/videos/400.gif" height="407" width="649">

After 700 iterations, the agent knows how to land.

<img src="/images/PPO-lunarlander/videos/781.gif" height="407" width="649">

As we can see from the leanring curve, the learned policy is unstable in this case. Furthermore, the optimal solution for lunar lander is getting average reward of 200 over 100 consecutive trials

<img src="/images/PPO-lunarlander/imgs/ppo2-1run-lunarlander-continuous.png" height="450" width="700">

And the results for 5 runs:

<img src="/images/PPO-lunarlander/imgs/ppo2-5runs-lunarlander.png" height="500" width="700">


## 1. Bipedal Walker-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95\\

### Results
Initally the agent takes random actions:
<img src="/images/PPO-bipedalwalker/videos/1.gif" height="407" width="649">

After 100 iterations, the agent starts to learn how to walk:

<img src="/images/PPO-bipedalwalker/videos/100.gif" height="407" width="649">

After 400 iterations, the agent could walk for a while:

<img src="/images/PPO-bipedalwalker/videos/400.gif" height="407" width="649">

After 700 iterations, the agent could walk for a while but couldn't achieve the optimal solution, which is getting average reward of 300 over 100 consecutive trials.

<img src="/images/PPO-bipedalwalker/videos/781.gif" height="407" width="649">

As we can see from the leanring curve, the learned policy is unstable in this case.

<img src="/images/PPO-bipedalwalker/imgs/ppo2-1run-bipedalwalker.png" height="450" width="700">

And the results for 5 runs:

<img src="/images/PPO-bipedalwalker/imgs/ppo2-5runs-bipedalwalker.png" height="500" width="700">

