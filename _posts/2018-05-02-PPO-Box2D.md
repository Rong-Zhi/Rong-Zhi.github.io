---
layout: post
title: PPO Gym env
date: 2018-05-02
excerpt: "Here are some PPO implmentation results in OpenAI Gym environments"
tags: [Reinforcement Learning]
comments: true
markdown: kramdown
mathjax: true
---

Here are some PPO implementation results in OpenAI Gym environments. [OpenAI baselines](https://github.com/openai/baselines) gave us two different versions of implementing [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) from Schulman et al.. PPO1 is designed for multi-cpu using MPI, while PPO2 is optimized for GPU. Note that the core algorithms for both versions are exactly the same, only the implementation way is slitely different. In our case, PPO1 average return always sticks to extremely low values, -130 for LunarLanderContinuous-v2, -100 for BipedalWalker-v2, PPO2 could achieve much better results, but still, not the optimal solution.

## 1. LunarLander Continuous-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95

### Results
Initally the agent takes random actions:
<img src="/images/PPO-lunarlander/videos/1.gif" style="width: 400px;">

After 100 iterations, the agent starts to learn how to land:

<img src="/images/PPO-lunarlander/videos/100.gif" style="width: 400px;">

After 400 iterations, the agent could land successfully:

<img src="/images/PPO-lunarlander/videos/400.gif" style="width: 400px;">

After 700 iterations, the agent knows how to land.

<img src="/images/PPO-lunarlander/videos/781.gif" style="width: 400px;">

As we can see from the leanring curve, our average return is around 100, while the [optimal solution](https://gym.openai.com/envs/LunarLanderContinuous-v2/) is getting average return of 200 over 100 consecutive trials

<img src="/images/PPO-lunarlander/imgs/ppo2-1run-lunarlander-continuous.png" style="width: 700px;">

And the results for 5 runs:

<img src="/images/PPO-lunarlander/imgs/ppo2-5runs-lunarlander.png" style="width: 700px;">

### Analysis

We couldn't find the reasons based on current results. So let's take a look at the environment code of how to define the reward function

```python
     state = [
            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),
            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_W/SCALE/2),
            vel.x*(VIEWPORT_W/SCALE/2)/FPS,
            vel.y*(VIEWPORT_H/SCALE/2)/FPS,
            self.lander.angle,
            20.0*self.lander.angularVelocity/FPS,
            1.0 if self.legs[0].ground_contact else 0.0,
            1.0 if self.legs[1].ground_contact else 0.0
            ]
        assert len(state)==8

        reward = 0
        shaping = \
            - 100*np.sqrt(state[0]*state[0] + state[1]*state[1]) \
            - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \
            - 100*abs(state[4]) + 10*state[6] + 10*state[7]   # And ten points for legs contact, the idea is if you
                                                              # lose contact again after landing, you get negative reward
        if self.prev_shaping is not None:
            reward = shaping - self.prev_shaping
        self.prev_shaping = shaping

        reward -= m_power*0.30  # less fuel spent is better, about -30 for heurisic landing
        reward -= s_power*0.03

        done = False
        if self.game_over or abs(state[0]) >= 1.0:
            done   = True
            reward = -100
        if not self.lander.awake:
            done   = True
            reward = +100
```

The shaping function could be written as 
\\[ shapeing=-100(x^2+y^2)^2-100(V_x^2+V_y^2)^2-\mid100\theta\mid+10L_l+10L_r \\] 
\\[ reward=shaping-prevshaping-0.3mpower-0.3spower \\] 
the starting point is always at coordinates (0,1) with random velocity, theta=0, L_l=0, L_r=0. Episode finishes if the lander crushes or comes to rest, receiving -100 or +100 reward. We first change reward=-1 for the first case (crush) and keep lander.awake to +100, denoted as 'PPO-llm', and then change reward=+1 when it turns off the engine and keep game_over (crush) to -100, denoted as 'PPO-awake'.
<img src="/images/PPO-lunarlander/imgs/ppo2-lunarlander-compare.png" style="width: 700px;">
The blue curve is PPO with original environment setting.\\
After 700 iterations in first case ('PPO-llm'):
<img src="/images/PPO-lunarlander/videos/700-llm.gif" style="width: 400px;">

After 700 iteration in second case ('PPO-awake'):
<img src="/images/PPO-lunarlander/videos/700-awake.gif" style="width: 400px;">

Without large punishment for crush, the lander is capable to turn off the engine , similarly, without large reward for coming to rest, it learns how to land and get the average return around 100 as well. It seems like our PPO algorithm is stuck to local optimal solution.

To prevent premature convergence, we enabled additional entropy regularization term as bonus to surrogate loss function with constant coefficient.
\\[ L(\theta)=E[\dfrac{\pi(a|s)}{\pi_{old}(a|s)}A + c_{ent}H(\pi(a|s))] \\] 
We can see the 5-runs resutls of 2e6 timesteps (960 iterations) when c_ent=0.01 below, we set them into 10 epochs and 15 epochs respectivley:
<img src="/images/PPO-lunarlander/imgs/ppo2-10ep-15ep-compare.png" style="width: 800px;">
Now we are closer to the optimal solution, 15 epochs outperforms 10 epochs slitely in this case, if we run it for more timesteps(3e6), we could get the following results.
<img src="/images/PPO-lunarlander/imgs/ppo2-epochcompare2.png" style="width: 700px;">

If we modify the entropy coefficient term into linear coefficient term 
\\[ C_{ent}=max(0.1 - 0.1 \frac{i_{iters}}{N_{iters}}, 0.01) \\] 
we have even better result.

<img src="/images/PPO-lunarlander/imgs/ppo2-entropy-dynamic.png" style="width: 700px;">


## 2. Bipedal Walker-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95

### Results
Initally the agent takes random actions:
<img src="/images/PPO-bipedalwalker/videos/1.gif" style="width: 400px;">

After 100 iterations, the agent starts to learn how to walk:

<img src="/images/PPO-bipedalwalker/videos/100.gif" style="width: 400px;">

After 400 iterations, the agent could walk for a while:

<img src="/images/PPO-bipedalwalker/videos/400.gif" style="width: 400px;">

After 700 iterations, the agent could walk for a while but couldn't achieve the [optimal solution](https://github.com/openai/gym/wiki/Leaderboard), which is getting average reward of 300 over 100 consecutive trials.

<img src="/images/PPO-bipedalwalker/videos/781.gif" style="width: 400px;">

As we can see from the leanring curve, the learned policy is unstable in this case.

<img src="/images/PPO-bipedalwalker/imgs/ppo2-1run-bipedalwalker.png" style="width: 700px;">

And the results for 5 runs, it has converged to a local optimum solution.

<img src="/images/PPO-bipedalwalker/imgs/ppo2-5runs-bipedalwalker.png" style="width: 700px;">


We will mainly focus on the Lunar Lander Environment of further research and will not update any experiments of bipedal walker.

## 3. Tips
One shortcome of PPO and TRPO algorithm is the agent has to collect a large number of samples in each batch, which extremely slows down the entire experiments. To tackle this problem, we parallelize the sampling procedure via using 16 CPUs, each of them only needs 128 samples, we collect the data later on for gradient calculation and parameters update. Now we could run it faster on cluster! Cheers!

## 4. Next Blog
- Modify  Lunar Lander Continuous env to Partial observable env :)
