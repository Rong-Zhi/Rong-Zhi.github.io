---
layout: post
title: PPO Gym env
date: 2018-05-02
excerpt: "Here are some PPO implmentation results in OpenAI Gym environments"
tags: [Reinforcement Learning]
comments: true
markdown: kramdown
mathjax: true
---

Here are some PPO implementation results in OpenAI Gym environments. [OpenAI baselines](https://github.com/openai/baselines) gave us two different versions of implementing [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) from Schulman et al.. PPO1 is designed for multi-cpu using MPI, while PPO2 is optimized for GPU. Note that the core algorithms for both versions are exactly the same, only the implementation way is slitely different. In our case, PPO1 average return always sticks to extremely low values, -130 for LunarLanderContinuous-v2, -100 for BipedalWalker-v2, PPO2 could achieve much better results but not optimal solution.

## 1. LunarLander Continuous-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95

### Results
Initally the agent takes random actions:
<img src="/images/PPO-lunarlander/videos/1.gif" height="407" width="649">

After 100 iterations, the agent starts to learn how to land:

<img src="/images/PPO-lunarlander/videos/100.gif" height="407" width="649">

After 400 iterations, the agent could land successfully:

<img src="/images/PPO-lunarlander/videos/400.gif" height="407" width="649">

After 700 iterations, the agent knows how to land.

<img src="/images/PPO-lunarlander/videos/781.gif" height="407" width="649">

As we can see from the leanring curve, our average return is around 100, while the [optimal solution](https://gym.openai.com/envs/LunarLanderContinuous-v2/) is getting average return of 200 over 100 consecutive trials

<img src="/images/PPO-lunarlander/imgs/ppo2-1run-lunarlander-continuous.png" height="450" width="700">

And the results for 5 runs:

<img src="/images/PPO-lunarlander/imgs/ppo2-5runs-lunarlander.png" height="500" width="700">


## 2. Bipedal Walker-V2
### Hyperparameters
Neural Network: 2 hidden layers, each layer consists of 64 neurons\\
Number of steps per iteration: 2048\\
Total Iteration: 740\\
Minibatch size: 32\\
Learning rate: 3e-4\\
Clip range: 0.2\\
Discount factor: 0.99\\
lambda: 0.95

### Results
Initally the agent takes random actions:
<img src="/images/PPO-bipedalwalker/videos/1.gif" height="407" width="649">

After 100 iterations, the agent starts to learn how to walk:

<img src="/images/PPO-bipedalwalker/videos/100.gif" height="407" width="649">

After 400 iterations, the agent could walk for a while:

<img src="/images/PPO-bipedalwalker/videos/400.gif" height="407" width="649">

After 700 iterations, the agent could walk for a while but couldn't achieve the [optimal solution](https://github.com/openai/gym/wiki/Leaderboard), which is getting average reward of 300 over 100 consecutive trials.

<img src="/images/PPO-bipedalwalker/videos/781.gif" height="407" width="649">

As we can see from the leanring curve, the learned policy is unstable in this case.

<img src="/images/PPO-bipedalwalker/imgs/ppo2-1run-bipedalwalker.png" height="450" width="700">

And the results for 5 runs, it has converged to a local optimum solution.

<img src="/images/PPO-bipedalwalker/imgs/ppo2-5runs-bipedalwalker.png" height="500" width="700">


## 3. Next step
- Enable dynamic entropy term in PPO and run in the same env
- Modify env parameters to Partial observable
