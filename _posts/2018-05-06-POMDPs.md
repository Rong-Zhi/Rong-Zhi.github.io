---
layout: post
title: POMDPs env
date: 2018-05-06
excerpt: "Partial Observable Environment"
tags: [Reinforcement Learning]
comments: true
markdown: kramdown
mathjax: true
---

In MDP(Markov Decision Process) we assume that we can always observe the true state of environment and the next state of the environment is only dependent on the current state and chosen action. 
\\[ P_a(s,s') = P(s_{t+1}=s' \mid s_t = s, a_t=a) \\] 
With this assumtion, we could only take the current state to compute the next action and the result still be optimal.\\
In the real world, such assumption often does not hold for applications in the real world. The robos always perceive the world through noisy sensors and can only see parts of the world through a camera, which could be seen as POMDPS. The agent indirectly observes the environment's state via observations, and must make decisions under uncertainty of the true environment state, thus, the history observation has to be taken into account.

## Environment
We use [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2/) from OpenAI Gym as our experiment environment, and moidy it into a partial observalbe environment. 

In the original environment:

- The lander can get 8 dim of observation space, including its position, velocity, angular, angular velocity, and ground touch situation. 
- Landing pad is always at coordinates (0,0). 
- Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. 
- If lander moves away from landing pad it loses reward back. 
- Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. 
- Each leg ground contact is +10. Firing main engine is -0.3 points each frame. 
- Solved is 200 points.
<img src="/images/PPO-lunarlander/videos/1.gif" style="width: 400px;">

Modified envrionment:

- We keep most of the settings above, build a fixed signal shelter area, and add an alarm sensor data to warn the lander of entering into this area, namely, -1 when the lander enters into this area, +1 when it is outside. The shelter area could be flexible determined for now, and would be randomly genereated in future.
- The lander can't observe any abovementioned sensor data except the alarm data, other observation remains the last collected data before it enter into this area. It can take actions as usual, and get the corresponding rewards.
- When it goes outside of the area, the environment settings return to original env.
<img src="/images/POMDP/videos/example.gif" style="width: 400px;">
The gray band represents the signal shelter area.
- step(action) will return a history_length * (observation_dim + action_dim) dimension array as the history data h1=(o1, a1, o2, a2, ... , on, an), where n=history_length


## Algorithm
We will implement different reinforcement learning algorihtms with histroy observation as input. 

- PPO with and without entropy regularization

## Experiments
We will test the following parameters of different PPO versions

| Method        |Hist len| Block Area |
|:--------------|:-------:|--------:|
| Clip          | 0  | 0.5H-0.5325H |
| Entropy-const | 10 | 0.5H-0.5625H |
|Entropy-dynamic| 20 | 0.5H-0.625H  |

<img src="/images/POMDP/imgs/clip-3l-bh17-histcompare.png" style="width: 700px;">
PPO clip with block hight 0.5H-0.5325H, compare performance with history length 0, 10, 20
<img src="/images/POMDP/imgs/ent2-2l-bh17-histcompare.png" style="width: 700px;">
PPO constant entropy with block hight 0.5H-0.5325H, compare performance with history length 0, 10, 20
<img src="/images/POMDP/imgs/ent-dynamic-bh17-histcompare.png" style="width: 700px;">
PPO dynamic entropy with same block hight 0.5H-0.5325H, compare performance with history length 0, 10, 20
<img src="/images/POMDP/imgs/ent-dynamic-bh9-histcompare.png" style="width: 700px;">
PPO dynamic entropy with same block hight 0.5H-0.5625H, compare performance with history length 0, 10, 20
<img src="/images/POMDP/imgs/ent-dynamic-bh5-histcompare.png" style="width: 700px;">
PPO dynamic entropy with same block hight 0.5H-0.625H, compare performance with history length 0, 10, 20
<img src="/images/POMDP/imgs/ent-dynamic-hist0-bhcompare.png" style="width: 700px;">
PPO dynamic entropy with same history length=0, compare performance with different block area size 


As we can see from above curves, the performance became poorer as history length increases, all algorithms worked better for smaller block area, 
and PPO with entropy dynamic outperforms else algorithms. 

## Questions
-- Why history info doesn't help in this case?